{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAgger on Part of Speech tagging\n",
    "\n",
    "This notebook shows how to run the imitation learning algorithm DAgger (Dataset Aggregation, [Ross et al. (2011)](https://arxiv.org/pdf/1011.0686.pdf)) on a toy part of speech tagging dataset, showcasing its benefits. It follows the terminology of the EACL 2017 tutorial on imitation learning for structured prediction ([Vlachos et al. 2017](http://sheffieldnlp.github.io/ImitationLearningTutorialEACL2017/)) and the code from this [github repository](http://github.com/andreasvlachos/structured_imitation_demo). The latter uses [scikit-learn](http://scikit-learn.org/stable/) classifiers in Python3 to faciliate adoptions by academic researchers and software developers. The notebook follows closely the code in this [file](http://github.com/andreasvlachos/structured_imitation_demo/blob/master/src/POSdemo.py), if you would rather go straight there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows we show how to do this step-by-step. First get the code from the [github repository](http://github.com/andreasvlachos/structured_imitation_demo) and import the library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import imitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the (typically structured) input and the structured output, combined in an instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class POSInput(imitation.StructuredInput):\n",
    "    def __init__(self, tokens):\n",
    "        self.tokens = tokens  \n",
    "    \n",
    "    # this is just to help us print things\n",
    "    def __str__(self):\n",
    "        return \" \".join(self.tokens)\n",
    "\n",
    "class POSOutput(imitation.StructuredOutput):\n",
    "    def __init__(self, tags=None):\n",
    "        self.tags = []\n",
    "        if tags!=None:\n",
    "            self.tags = tags\n",
    "            \n",
    "    # this is just to help us print things\n",
    "    def __str__(self):\n",
    "        return \" \".join(self.tags)\n",
    "            \n",
    "\n",
    "class POSInstance(imitation.StructuredInstance):\n",
    "    def __init__(self, tokens, tags=None):\n",
    "        super().__init__()\n",
    "        self.input = POSInput(tokens)\n",
    "        self.output = POSOutput(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the work is defining the transition system. The package has a class ```TransitionSystem``` that helps define it. See the comments in the code for some hints about its construction: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class POSTransitionSystem(imitation.TransitionSystem):\n",
    "\n",
    "    class WordAction(imitation.TransitionSystem.Action):\n",
    "        def __init__(self):\n",
    "            # The superclass constructor initializes the label and the features that each action has\n",
    "            super().__init__()\n",
    "\n",
    "    # the agenda for word prediction is one action per token, left-to-right\n",
    "    def __init__(self, structured_instance=None):\n",
    "        super().__init__(structured_instance)\n",
    "        if structured_instance == None:\n",
    "            return\n",
    "        for tokenNo, token in enumerate(structured_instance.input.tokens):\n",
    "            newAction = self.WordAction()\n",
    "            newAction.tokenNo = tokenNo\n",
    "            self.agenda.append(newAction)\n",
    "\n",
    "    # the expert policy is trivial in the case of PoS tagging: just return the correct label from gold\n",
    "    def expert_policy(self, structured_instance, action):\n",
    "        # just return the next action\n",
    "        return structured_instance.output.tags[action.tokenNo]\n",
    "\n",
    "    # Here we could be doing more book-keeping to help extract more complex features\n",
    "    def updateWithAction(self, action, structuredInstance):\n",
    "        # add it as an action though\n",
    "        self.actionsTaken.append(action)\n",
    "\n",
    "    # The feature engineering goes here\n",
    "    def extractFeatures(self, structured_instance, action):\n",
    "        # e.g the word itself that we are tagging\n",
    "        features = {\"currentWord=\" + structured_instance.input.tokens[action.tokenNo]: 1}\n",
    "\n",
    "        # features based on the previous predictionsof this stage are to be accessed via the self.actionsTaken\n",
    "        # e.g. the previous action\n",
    "        if len(self.actionsTaken) > 0:\n",
    "            features[\"prevPrediction=\" + self.actionsTaken[-1].label] = 1\n",
    "        else:\n",
    "            features[\"prevPrediction=NULL\"] = 1\n",
    "\n",
    "        return features\n",
    "\n",
    "    # Convert the action sequence in the state to the actual prediction, i.e. a sequence of tags\n",
    "    def to_output(self):\n",
    "        tags = []\n",
    "        for action in self.actionsTaken:\n",
    "            tags.append(action.label)\n",
    "        return POSOutput(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is it! We can now write the following which specifies that our tagger will be learned by the ```ImitationLearner``` with the ```POSTransitionSystem```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class POSTagger(imitation.ImitationLearner):\n",
    "    # specify the transition system\n",
    "    transitionSystem = POSTransitionSystem\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a toy dataset and an instance of the tagger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingInstances = []\n",
    "\n",
    "# two instances\n",
    "trainingInstances.extend([POSInstance([\"I\", \"can\", \"fly\"], [\"Pronoun\", \"Modal\", \"Verb\"])])\n",
    "trainingInstances.extend([POSInstance([\"I\", \"can\", \"meat\"], [\"Pronoun\", \"Verb\", \"Noun\"])])\n",
    "\n",
    "# repeated multiple times\n",
    "trainingInstances.extend(30*[POSInstance([\"I\", \"can\", \"fly\"], [\"Pronoun\", \"Modal\", \"Verb\"])])\n",
    "trainingInstances.extend(10*[POSInstance([\"I\", \"can\", \"meat\"], [\"Pronoun\", \"Verb\", \"Noun\"])])\n",
    "\n",
    "tagger = POSTagger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the features (current word, previous tag), the classifier receives an ambiguous training signal. Thus it will learn to predict only one of the two cases of \"can\" correctly, the one with more appearances in the training data. Of course more complex features could address this, but generally speaking we rarely (want to) have features that are too complex as they tend to be sparse and not generalize. Let's see what happens here when training with 1 iteration of DAgger, which is the equivalent of standard supervised training, also referred to as exact imitation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:0, expert policy prob:1.0\n",
      "I can fly\n",
      "Pronoun Modal Verb\n",
      "I can meat\n",
      "Pronoun Modal Verb\n"
     ]
    }
   ],
   "source": [
    "params = POSTagger.params()\n",
    "params.iterations = 1\n",
    "\n",
    "tagger.train(trainingInstances, params)\n",
    "\n",
    "print(trainingInstances[0].input)\n",
    "print(tagger.predict(trainingInstances[0]).to_output())\n",
    "print(trainingInstances[1].input)\n",
    "print(tagger.predict(trainingInstances[1]).to_output())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we cannot learn that \"can\" can have different tags depending on the context, but also we get another error that \"meat\" is tagged as a verb! This is happening because, even though we always saw \"meat\" only as a noun in our data, we also saw more instances of the tag \"Modal\" followed by a \"Verb\" tag. While this is a useful pattern to learn, we never encountered cases where the \"Modal\" tag is actually a mistake, since the training data labels are (typically) correct. In other words, we need to expose our model to incorrect predictions, so that it learns to recover from them. This is what we will do by adding a second iteration of DAgger, in which the roll-in will be with the classifier exclusively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:0, expert policy prob:1\n",
      "Iteration:1, expert policy prob:0\n",
      "I can fly\n",
      "Pronoun Modal Verb\n",
      "I can meat\n",
      "Pronoun Modal Noun\n"
     ]
    }
   ],
   "source": [
    "paramsImit = POSTagger.params()\n",
    "paramsImit.iterations = 2\n",
    "# The formula for probability of using the expert policy per iteration is (1-params.learningParam)^iteration_no\n",
    "# iteration_no starts from 0, so 1 in the first iteration and 1-learningParam in the second one\n",
    "paramsImit.learningParam = 1 \n",
    "\n",
    "tagger.train(trainingInstances, paramsImit)\n",
    "\n",
    "print(trainingInstances[0].input)\n",
    "print(tagger.predict(trainingInstances[0]).to_output())\n",
    "print(trainingInstances[1].input)\n",
    "print(tagger.predict(trainingInstances[1]).to_output())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model now has learned to avoid the second mistake of tagging \"meat\" as a \"Verb\", thanks to the additional training data generated in the second generation of DAgger. Of course more features could have achieved this too, but we are unlikely to have the perfect classifier, so it is better to prepare it for its own mistakes.\n",
    "\n",
    "I intend to keep working on improving this tutorial and codebase to add the notion of roll-outs and training against non-decomposoable losses. If you see any mistakes, or you have any questions or requests, I would be more than happy to hear them: [a.vlachos@sheffield.ac.uk](mailto:a.vlachos@sheffield.ac.uk)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgments\n",
    "\n",
    "I have been working on imitation learning for structured prediction for many years with many great collaborators. However, special thanks are due to Gerasimos Lampouras and Sebastian Riedel who worked with me on preparing the aforementioned EACL 2017 tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
